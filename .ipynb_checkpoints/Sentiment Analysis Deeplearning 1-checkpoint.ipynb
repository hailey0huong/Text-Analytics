{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Deeplearning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\usr-profiles\\chuang\\AppData\\Local\\conda\\conda\\envs\\tf13\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os \n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "1420\n"
     ]
    }
   ],
   "source": [
    "labeled_news1 = pd.read_csv('Full-Economic-News-DFE-839861.csv',encoding = 'ISO-8859-1')\n",
    "train_data1 = labeled_news1.loc[labeled_news1.relevance == 'yes']\n",
    "train_data1 = train_data1[['text','positivity','positivity:confidence']]\n",
    "print(len(labeled_news1))\n",
    "print(len(train_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5015\n",
      "2901\n"
     ]
    }
   ],
   "source": [
    "labeled_news2 = pd.read_csv('us-economic-newspaper.csv',encoding = 'ISO-8859-1')\n",
    "train_data2 = labeled_news2.loc[labeled_news2.relevance == 'yes']\n",
    "train_data2 = train_data2[['text','positivity','positivity:confidence']]\n",
    "print(len(labeled_news2))\n",
    "print(len(train_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4321\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([train_data1, train_data2])\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4319, 3)\n"
     ]
    }
   ],
   "source": [
    "# Drop missing values\n",
    "train_data.dropna(inplace=True)\n",
    "print (train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any 'neutral' ratings equal to 5\n",
    "train_data = train_data.loc[train_data['positivity'] != 5]\n",
    "\n",
    "# Encode 6-9s as 1 (rated positively)\n",
    "# Encode 1-4s as 0 (rated poorly)\n",
    "train_data['Positively Rated'] = np.where(train_data['positivity'] > 5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3888"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The numbers of positive rating and negative rating are quite even\n",
    "train_data['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text( raw_news, lemmatizer, stopw ):\n",
    "    '''\n",
    "    Function to convert a raw news to a string of words\n",
    "    The input is a single string (a raw news), and \n",
    "    the output is a single string (a preprocessed news)\n",
    "    '''\n",
    "    from bs4 import BeautifulSoup  \n",
    "    \n",
    "    # Remove HTML\n",
    "    news_text = BeautifulSoup(raw_news,\"lxml\").get_text() \n",
    "\n",
    "    # Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", news_text) \n",
    "    # Tokenize and clean bag of words:\n",
    "    tokens = word_tokenize(letters_only.lower())\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    #tokens = [t for t in tokens if t not in stopw]\n",
    "    #tokens = [t for t in tokens if len(t)>1]\n",
    "    \n",
    "    return tokens   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Apply this to the dataframe\n",
    "train_data['processed_text'] = train_data['text'].apply(lambda t: process_text(t, lemmatizer, stopw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>Positively Rated</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0</td>\n",
       "      <td>[new, york, yield, on, most, certificate, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>0</td>\n",
       "      <td>[new, york, indecision, marked, the, dollar, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  positivity  \\\n",
       "0  NEW YORK -- Yields on most certificates of dep...         3.0   \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...         3.0   \n",
       "\n",
       "   positivity:confidence  Positively Rated  \\\n",
       "0                 0.6400                 0   \n",
       "4                 0.3257                 0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [new, york, yield, on, most, certificate, of, ...  \n",
       "4  [new, york, indecision, marked, the, dollar, s...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data['processed_text'], \n",
    "                                                    train_data['Positively Rated'], \n",
    "                                                    random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec and Doc2Vec Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use pretrained w2v\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_google = True\n",
    "\n",
    "if use_google:\n",
    "    news_w2v = KeyedVectors.load_word2vec_format(os.path.join('pre_trained_w2v','GoogleNews-vectors-negative300.bin'), binary=True)\n",
    "else:\n",
    "    news_w2v = Word2Vec.load(os.path.join('pre_trained_w2v','imf_160.w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7291510105133057),\n",
       " ('bad', 0.7190051078796387),\n",
       " ('terrific', 0.6889116168022156),\n",
       " ('decent', 0.6837348341941833),\n",
       " ('nice', 0.6836092472076416),\n",
       " ('excellent', 0.644292950630188),\n",
       " ('fantastic', 0.6407778263092041),\n",
       " ('better', 0.6120728254318237),\n",
       " ('solid', 0.5806034803390503),\n",
       " ('lousy', 0.5764201879501343)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_w2v.wv.most_similar('good',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build word vector for training set by using the average value of all word vectors in the news, then scale\n",
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += news_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "n_dim = 300\n",
    "\n",
    "train_vecs = np.concatenate([buildWordVector(z, n_dim) for z in X_train])\n",
    "train_vecs = scale(train_vecs)\n",
    "\n",
    "#Build test news vectors then scale\n",
    "test_vecs = np.concatenate([buildWordVector(z, n_dim) for z in X_test])\n",
    "test_vecs = scale(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.70302345157\n",
      "Accuracy:  0.683368869936\n"
     ]
    }
   ],
   "source": [
    "# Try Logistic Regression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "modellr = LogisticRegression()\n",
    "modellr.fit(train_vecs, y_train)\n",
    "\n",
    "predictions = modellr.predict_proba(test_vecs)[:,1]\n",
    "exact_predict = modellr.predict(test_vecs)\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('Accuracy: ', accuracy_score(y_test, exact_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense = keras.layers.Dense\n",
    "Dropout = keras.layers.Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(units=200,activation='sigmoid',input_dim=300))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=100,activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    ## output layer\n",
    "    model.add(Dense(units=2,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = keras.utils.to_categorical(y_train,num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4a203255f8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_vecs, y_train_one_hot, epochs=400, batch_size=32,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812/2812 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.54573713571553883, 0.71941678495190053]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(train_vecs, y_train_one_hot)\n",
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 0s 28us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.58264108520072655, 0.70149253744052165]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one_hot = keras.utils.to_categorical(y_test,num_classes=None)\n",
    "loss_and_metrics = model.evaluate(test_vecs, y_test_one_hot)\n",
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 300\n",
    "n_classes = 2 \n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batch_size = 32\n",
    "keep_rate = 0.1\n",
    "\n",
    "save_file = 'ckpt/train_model.ckpt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## keep probability for drop out\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "## input layer \n",
    "x = tf.placeholder(\"float\",[None,n_input])      ## because out data is in grey scale, so only has 1 channel \n",
    "y = tf.placeholder(\"float\",[None,n_classes])\n",
    "\n",
    "\n",
    "## hiden layer\n",
    "layer1 = tf.layers.dense(inputs=x, units=128,activation=tf.nn.sigmoid)\n",
    "layer1 = tf.nn.dropout(layer1,keep_prob) \n",
    "layer2 = tf.layers.dense(inputs=layer1, units=32,activation=tf.nn.sigmoid)\n",
    "layer2 = tf.nn.dropout(layer2,keep_prob) \n",
    "\n",
    "## output layer \n",
    "logits = tf.layers.dense(inputs=layer2, units=2)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "## calculate accuracy\n",
    "pred_probas = tf.nn.softmax(logits)\n",
    "pred_classes = tf.argmax(logits, axis=1)\n",
    "\n",
    "correct_prediction = tf.equal(pred_classes,tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    # Get the batch size and number of batches we can make\n",
    "    n_batches = len(X)//batch_size     ## only keep the integer number \n",
    "    for n in range(0, n_batches):\n",
    "        # The features\n",
    "        x = np.array(X[n*batch_size:n*batch_size+batch_size])\n",
    "        # The targets, shifted by one\n",
    "        y = np.array(Y[n*batch_size:n*batch_size+batch_size])\n",
    "        yield x, y\n",
    "    if len(X)//batch_size>0:\n",
    "        x = np.array(X[(n_batches-1)*batch_size:])\n",
    "        y = np.array(Y[(n_batches-1)*batch_size:])\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Cost: 1.273625, Training Accuracy: 0.604, Validation Accuracy: 0.633\n",
      "Epoch 50  - Cost: 0.618965, Training Accuracy: 0.604, Validation Accuracy: 0.633\n",
      "Epoch 100 - Cost: 0.548313, Training Accuracy: 0.729, Validation Accuracy: 0.716\n",
      "Epoch 150 - Cost: 0.541274, Training Accuracy: 0.744, Validation Accuracy: 0.713\n",
      "Epoch 200 - Cost: 0.554368, Training Accuracy: 0.762, Validation Accuracy: 0.714\n",
      "Epoch 250 - Cost: 0.534694, Training Accuracy: 0.768, Validation Accuracy: 0.714\n",
      "Epoch 300 - Cost: 0.560570, Training Accuracy: 0.781, Validation Accuracy: 0.710\n",
      "Epoch 350 - Cost: 0.635540, Training Accuracy: 0.789, Validation Accuracy: 0.710\n",
      "Epoch 400 - Cost: 0.548678, Training Accuracy: 0.793, Validation Accuracy: 0.707\n",
      "Epoch 450 - Cost: 0.448937, Training Accuracy: 0.798, Validation Accuracy: 0.705\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "batches = list(get_batches(train_vecs, y_train_one_hot, batch_size))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)                           ## run initializer\n",
    "    ## train cycles \n",
    "    epoch = None\n",
    "    valid_accuracy = None\n",
    "    #epoch_pbar = tqdm(range(training_epochs), desc='Epoch: {}, Validation Accuracy: {}'.format(epoch, valid_accuracy), unit='epoches')\n",
    "    for epoch in range(training_epochs):   ## use tqdm for process bar \n",
    "        total_batch = len(train_vecs)//batch_size\n",
    "        ## loop over all batches \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batches[i]\n",
    "            _,loss = sess.run([train_op,cost], feed_dict={x: batch_x, y: batch_y,keep_prob: keep_rate})\n",
    "    \n",
    "        # Calculate Training and Validation accuracy\n",
    "        training_accuracy = sess.run(accuracy,feed_dict={\n",
    "                    x: train_vecs,\n",
    "                    y: y_train_one_hot,\n",
    "                    keep_prob: 1.0})\n",
    "        \n",
    "        validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "                    x: test_vecs,\n",
    "                    y: y_test_one_hot,\n",
    "                    keep_prob: 1.0})\n",
    "        # Log accuracy\n",
    "        loss_list.append(loss)\n",
    "        train_acc_list.append(training_accuracy)\n",
    "        valid_acc_list.append(validation_accuracy)\n",
    "                               \n",
    "        ## print status for every 10 epochs \n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch {:<3} - Cost: {:.6f}, Training Accuracy: {:.3f}, Validation Accuracy: {:.3f}'.format(\n",
    "                epoch,\n",
    "                loss,\n",
    "                training_accuracy,\n",
    "                validation_accuracy))\n",
    "            \n",
    "    ####################################\n",
    "    ## save the model for future use####\n",
    "    ####################################\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reload graph for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt/train_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "#graph = tf.Graph()\n",
    "sess = tf.Session() \n",
    "new_saver = tf.train.import_meta_graph(save_file+'.meta')\n",
    "new_saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = sess.run(accuracy,feed_dict={\n",
    "            x: train_vecs,\n",
    "            y: y_train_one_hot,\n",
    "            keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80192035"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = sess.run(accuracy,feed_dict={\n",
    "            x: test_vecs,\n",
    "            y: y_test_one_hot,\n",
    "            keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70682305"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
